{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KPI Mock data creation for Batch 2 & 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Mock Data\n",
    "\n",
    "Daily Batch Feed 2\n",
    "Cherished Documents Received\n",
    "\n",
    "Includes the following columns:\n",
    "- cherish_doc_id\n",
    "- cherish_doc_type\n",
    "- customer_id\t\n",
    "- received_timestamp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    customer_id cherish_doc_id  received_timestamp      cherish_doc_type\n",
      "0  CUST00000001        CDOC001 2023-11-13 18:57:55              Passport\n",
      "1  CUST00000002        CDOC002 2023-09-19 14:52:21     Death_Certificate\n",
      "2  CUST00000003        CDOC003 2024-02-12 20:30:53     Death_Certificate\n",
      "3  CUST00000004        CDOC004 2024-01-13 07:09:57  Marriage_Certificate\n",
      "4  CUST00000005        CDOC005 2024-01-30 11:04:39     Death_Certificate\n",
      "Data saved to 'C:/Projects/NS&I/new_mockfiles/sps_cherisheddocumentsreceived.parquet'.\n",
      "customer_id                   object\n",
      "cherish_doc_id                object\n",
      "received_timestamp    datetime64[ns]\n",
      "cherish_doc_type              object\n",
      "dtype: object\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "# Mock data for Batch #2 - Cherished Documents Received\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "### Create Batch 2\n",
    "# Set conditions\n",
    "np.random.seed(0)\n",
    "filepath = 'C:/Projects/NS&I/new_mockfiles/'\n",
    "batch2_output_filename = filepath + 'sps_cherisheddocumentsreceived.parquet'\n",
    "num_samples = 3000\n",
    "\n",
    "# length_min_days = 1\n",
    "# length_max_days = 18\n",
    "cherish_doc_type_list = ['Income_Certificate','Death_Certificate','Marriage_Certificate','Passport','Birth_Certificate']\n",
    "\n",
    "customer_id = ['CUST' + f'{i:08}' for i in range(1, num_samples + 1)]\n",
    "cherish_doc_id = ['CDOC' + f'{i:03}' for i in range(1, num_samples + 1)]\n",
    "\n",
    "start_date = pd.to_datetime('2023-09-01')\n",
    "end_date = pd.to_datetime(datetime.now().strftime('%Y-%m-%d')) \n",
    "received_timestamp = pd.to_datetime(np.random.randint(start_date.value, end_date.value, num_samples,dtype='int64'), unit='ns')\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'customer_id': customer_id,\n",
    "    'cherish_doc_id': cherish_doc_id,\n",
    "    'received_timestamp': received_timestamp\n",
    "})\n",
    "\n",
    "df['cherish_doc_type'] = np.random.choice(cherish_doc_type_list, size=len(df))\n",
    "\n",
    "df['received_timestamp'] = pd.to_datetime(df['received_timestamp']).dt.round('s')\n",
    "df\n",
    "\n",
    "# random_days = pd.to_timedelta(np.random.randint(length_min_days, length_max_days, size=len(df)), unit='D')\n",
    "# df['closed_timestamp'] = df['created_timestamp'] + random_days\n",
    "# df['closed_timestamp'] = pd.to_datetime(df['closed_timestamp']).dt.round('s')\n",
    "\n",
    "# df['created_timestamp'] = df['created_timestamp'].astype(str)\n",
    "# df['closed_timestamp'] = df['closed_timestamp'].astype(str)\n",
    "\n",
    "\n",
    "df.to_parquet(batch2_output_filename, engine='pyarrow', index=False)\n",
    "\n",
    "print(df.head())\n",
    "print(f\"Data saved to '{batch2_output_filename}'.\")\n",
    "print(df.dtypes)\n",
    "print('------------------------')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3000 entries, 0 to 2999\n",
      "Data columns (total 4 columns):\n",
      " #   Column              Non-Null Count  Dtype         \n",
      "---  ------              --------------  -----         \n",
      " 0   customer_id         3000 non-null   object        \n",
      " 1   cherish_doc_id      3000 non-null   object        \n",
      " 2   received_timestamp  3000 non-null   datetime64[ns]\n",
      " 3   cherish_doc_type    3000 non-null   object        \n",
      "dtypes: datetime64[ns](1), object(3)\n",
      "memory usage: 93.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df_check = pd.read_parquet(batch2_output_filename)\n",
    "df_check.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Mock Data\n",
    "\n",
    "Daily Batch Feed 3\n",
    "Cherished Documents Despatched\n",
    "\n",
    "Includes the following columns:\n",
    "- cherish_doc_id\n",
    "- cherish_doc_type\n",
    "- customer_id\n",
    "- despatched_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    customer_id cherish_doc_id despatched_timestamp      cherish_doc_type\n",
      "0  CUST00000001        CDOC001  2023-11-14 16:57:55              Passport\n",
      "1  CUST00000002        CDOC002  2023-09-20 12:52:21     Death_Certificate\n",
      "2  CUST00000003        CDOC003  2024-02-13 18:30:53     Death_Certificate\n",
      "3  CUST00000004        CDOC004  2024-01-14 05:09:57  Marriage_Certificate\n",
      "4  CUST00000005        CDOC005  2024-01-31 09:04:39     Death_Certificate\n",
      "Data saved to 'C:/Projects/NS&I/new_mockfiles/sps_cherisheddocumentsdespatched.parquet'.\n",
      "customer_id                     object\n",
      "cherish_doc_id                  object\n",
      "despatched_timestamp    datetime64[ns]\n",
      "cherish_doc_type                object\n",
      "dtype: object\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "# Mock data for Batch #3 - Cherished Documents Despatched\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "### Create Batch 3\n",
    "# Set conditions\n",
    "np.random.seed(0)\n",
    "filepath = 'C:/Projects/NS&I/new_mockfiles/'\n",
    "batch3_output_filename = filepath + 'sps_cherisheddocumentsdespatched.parquet'\n",
    "num_samples = 3000\n",
    "\n",
    "length_min_days = 1\n",
    "length_max_days = 18\n",
    "cherish_doc_type_list = ['Income_Certificate','Death_Certificate','Marriage_Certificate','Passport','Birth_Certificate']\n",
    "\n",
    "customer_id = ['CUST' + f'{i:08}' for i in range(1, num_samples + 1)]\n",
    "cherish_doc_id = ['CDOC' + f'{i:03}' for i in range(1, num_samples + 1)]\n",
    "\n",
    "start_date = pd.to_datetime('2023-09-01')\n",
    "end_date = pd.to_datetime(datetime.now().strftime('%Y-%m-%d')) \n",
    "despatched_timestamp = pd.to_datetime(np.random.randint(start_date.value, end_date.value, num_samples,dtype='int64'), unit='ns')\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'customer_id': customer_id,\n",
    "    'cherish_doc_id': cherish_doc_id,\n",
    "    'despatched_timestamp': despatched_timestamp \n",
    "})\n",
    "\n",
    "df['cherish_doc_type'] = np.random.choice(cherish_doc_type_list, size=len(df))\n",
    "\n",
    "\n",
    "df['despatched_timestamp'] = pd.to_datetime(df['despatched_timestamp']).dt.round('s') + pd.Timedelta(hours=np.random.randint(20,50))\n",
    "df\n",
    "\n",
    "# random_days = pd.to_timedelta(np.random.randint(length_min_days, length_max_days, size=len(df)), unit='D')\n",
    "# df['closed_timestamp'] = df['created_timestamp'] + random_days\n",
    "# df['closed_timestamp'] = pd.to_datetime(df['closed_timestamp']).dt.round('s')\n",
    "\n",
    "# df['created_timestamp'] = df['created_timestamp'].astype(str)\n",
    "# df['closed_timestamp'] = df['closed_timestamp'].astype(str)\n",
    "\n",
    "\n",
    "df.to_parquet(batch3_output_filename, engine='pyarrow', index=False)\n",
    "\n",
    "print(df.head())\n",
    "print(f\"Data saved to '{batch3_output_filename}'.\")\n",
    "print(df.dtypes)\n",
    "print('------------------------')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'despatched_timestamp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'despatched_timestamp'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m df_check3 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(batch3_output_filename)\n\u001b[1;32m----> 2\u001b[0m df_check3[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdespatched_timestamp\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m pd\u001b[38;5;241m.\u001b[39mTimedelta(hours\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m20\u001b[39m,\u001b[38;5;241m50\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'despatched_timestamp'"
     ]
    }
   ],
   "source": [
    "df_check3 = pd.read_parquet(batch3_output_filename)\n",
    "df_check3['despatched_timestamp'] + pd.Timedelta(hours=np.random.randint(20,50))\n",
    "#df_check3[df_check3['despatched_timestamp'] + pd.Timedelta(hours=10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Mock Data\n",
    "\n",
    "Daily Batch Feed 5 Change Incidents\n",
    "\n",
    "Includes the following columns:\n",
    "- incident_id\n",
    "- date_of_incident\n",
    "- change_id\n",
    "- incident_priority\n",
    "- date_of_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   incident_id    change_id    date_of_incident      date_of_change  \\\n",
      "0  INC00000001  CHG00000001 2023-11-13 18:57:55 2024-02-21 01:13:29   \n",
      "1  INC00000002  CHG00000002 2023-09-19 14:52:21 2024-04-10 00:15:31   \n",
      "2  INC00000003  CHG00000003 2024-02-12 20:30:53 2023-12-04 08:18:53   \n",
      "3  INC00000004  CHG00000004 2024-01-13 07:09:57 2024-02-12 14:29:51   \n",
      "4  INC00000005  CHG00000005 2024-01-30 11:04:39 2023-11-24 23:31:37   \n",
      "\n",
      "   incident_priority  \n",
      "0                  2  \n",
      "1                  2  \n",
      "2                  1  \n",
      "3                  1  \n",
      "4                  1  \n",
      "Data saved to 'C:/Projects/NS&I/new_mockfiles/servicenow_changeincidents.parquet'.\n",
      "incident_id                  object\n",
      "change_id                    object\n",
      "date_of_incident     datetime64[ns]\n",
      "date_of_change       datetime64[ns]\n",
      "incident_priority             int32\n",
      "dtype: object\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "# Mock data for Batch #5 - Change Incidents\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "### Create Batch 5\n",
    "# Set conditions\n",
    "np.random.seed(0)\n",
    "filepath = 'C:/Projects/NS&I/new_mockfiles/'\n",
    "batch5_output_filename = filepath + 'servicenow_changeincidents.parquet'\n",
    "num_samples = 3000\n",
    "\n",
    "length_min_days = 1\n",
    "length_max_days = 18\n",
    "incident_priority_list = [1,2]\n",
    "\n",
    "incident_id = ['INC' + f'{i:08}' for i in range(1, num_samples + 1)]\n",
    "change_id = ['CHG' + f'{i:08}' for i in range(1, num_samples + 1)]\n",
    "\n",
    "start_date = pd.to_datetime('2023-09-01')\n",
    "end_date = pd.to_datetime(datetime.now().strftime('%Y-%m-%d')) \n",
    "date_of_incident = pd.to_datetime(np.random.randint(start_date.value, end_date.value, num_samples,dtype='int64'), unit='ns')\n",
    "date_of_change = pd.to_datetime(np.random.randint(start_date.value, end_date.value, num_samples,dtype='int64'), unit='ns') + pd.Timedelta(hours=np.random.randint(12,24))\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'incident_id': incident_id,\n",
    "    'change_id': change_id,\n",
    "    'date_of_incident': date_of_incident,\n",
    "    'date_of_change': date_of_change\n",
    "})\n",
    "\n",
    "df['incident_priority'] = np.random.choice(incident_priority_list, size=len(df))\n",
    "df['date_of_incident'] = pd.to_datetime(df['date_of_incident']).dt.round('s')\n",
    "df['date_of_change'] = pd.to_datetime(df['date_of_change']).dt.round('s')\n",
    "\n",
    "df\n",
    "\n",
    "df.to_parquet(batch5_output_filename, engine='pyarrow', index=False)\n",
    "\n",
    "print(df.head())\n",
    "print(f\"Data saved to '{batch5_output_filename}'.\")\n",
    "print(df.dtypes)\n",
    "print('------------------------')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Mock Data\n",
    "\n",
    "Daily Batch Feed 6 Call Analytics\n",
    "\n",
    "Includes the following columns:\n",
    "- calls_disconnected\n",
    "- created_timestamp\n",
    "- calls_answered_in_60_seconds\n",
    "- calls_received"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   calls_disconnected  calls_answered_in_60_seconds   created_timestamp  \\\n",
      "0                  49                           380 2023-11-13 18:57:55   \n",
      "1                  45                           383 2023-09-19 14:52:21   \n",
      "2                  47                           376 2024-02-12 20:30:53   \n",
      "3                  32                           388 2024-01-13 07:09:57   \n",
      "4                  39                           391 2024-01-30 11:04:39   \n",
      "\n",
      "   calls_received  \n",
      "0             429  \n",
      "1             428  \n",
      "2             423  \n",
      "3             420  \n",
      "4             430  \n",
      "Data saved to 'C:/Projects/NS&I/new_mockfiles/servicenow_changeincidents.parquet'.\n",
      "calls_disconnected                       int64\n",
      "calls_answered_in_60_seconds             int64\n",
      "created_timestamp               datetime64[ns]\n",
      "calls_received                           int64\n",
      "dtype: object\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "# Mock data for Batch #6 - nicecxone_callsanalytics\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "### Create Batch 5\n",
    "# Set conditions\n",
    "np.random.seed(0)\n",
    "filepath = 'C:/Projects/NS&I/new_mockfiles/'\n",
    "batch6_output_filename = filepath + 'nicecxone_callsanalytics.parquet'\n",
    "num_samples = 3000\n",
    "\n",
    "start_date = pd.to_datetime('2023-09-01')\n",
    "end_date = pd.to_datetime(datetime.now().strftime('%Y-%m-%d')) \n",
    "created_timestamp = pd.to_datetime(np.random.randint(start_date.value, end_date.value, num_samples,dtype='int64'), unit='ns')\n",
    "calls_disconnected = np.random.randint(30, 50, num_samples,dtype='int64')\n",
    "calls_answered_in_60_seconds = np.random.randint(350, 400, num_samples,dtype='int64')\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'calls_disconnected': calls_disconnected,\n",
    "    'calls_answered_in_60_seconds': calls_answered_in_60_seconds,\n",
    "    'created_timestamp': created_timestamp\n",
    "})\n",
    "\n",
    "df['created_timestamp'] = pd.to_datetime(df['created_timestamp']).dt.round('s')\n",
    "df['calls_received'] = df['calls_disconnected'] + df['calls_answered_in_60_seconds']\n",
    "\n",
    "df\n",
    "\n",
    "df.to_parquet(batch6_output_filename, engine='pyarrow', index=False)\n",
    "\n",
    "print(df.head())\n",
    "print(f\"Data saved to '{batch5_output_filename}'.\")\n",
    "print(df.dtypes)\n",
    "print('------------------------')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Mock Data\n",
    "\n",
    "Daily Batch Feed 7 sps_postoutrequests and 24 ppa_postoutbulkrequests\n",
    "\n",
    "7 Includes the following columns:\n",
    "- file_id\n",
    "- post_out_id\n",
    "- timestamp\n",
    "\n",
    "24 Includes the following columns:\n",
    "- post_out_type\n",
    "- file_received_timestamp\n",
    "- file_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       file_id   post_out_id           timestamp\n",
      "0  FID00000001  POID00000001 2023-11-13 18:57:54\n",
      "1  FID00000002  POID00000002 2023-09-19 14:52:21\n",
      "2  FID00000003  POID00000003 2024-02-12 20:30:52\n",
      "3  FID00000004  POID00000004 2024-01-13 07:09:57\n",
      "4  FID00000005  POID00000005 2024-01-30 11:04:39\n",
      "Data saved to 'C:/Projects/NS&I/new_mockfiles/sps_postoutrequests.parquet'.\n",
      "file_id                object\n",
      "post_out_id            object\n",
      "timestamp      datetime64[ns]\n",
      "dtype: object\n",
      "------------------------\n",
      "       file_id         post_out_type file_received_timestamp\n",
      "0  FID00000001           daily_files     2023-11-10 18:57:54\n",
      "1  FID00000002           daily_files     2023-09-16 14:52:21\n",
      "2  FID00000003           daily_files     2024-01-31 20:30:52\n",
      "3  FID00000004  Scheduled Statements     2024-01-04 07:09:57\n",
      "4  FID00000005  Scheduled Statements     2024-01-29 11:04:39\n",
      "Data saved to 'C:/Projects/NS&I/new_mockfiles/ppa_postoutbulkrequests.parquet'.\n",
      "file_id                            object\n",
      "post_out_type                      object\n",
      "file_received_timestamp    datetime64[ns]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Mock data for Batch #7 - Post Out Requests\n",
    "\n",
    "# Required fields:\n",
    "#   file_id\n",
    "#   post_out_id\n",
    "#   timestamp\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "### Create Batch 7\n",
    "# Set conditions\n",
    "np.random.seed(0)\n",
    "filepath = 'C:/Projects/NS&I/new_mockfiles/'\n",
    "batch7_output_filename = filepath + 'sps_postoutrequests.parquet'\n",
    "num_samples = 3000\n",
    "\n",
    "file_ids = ['FID' + f'{i:08}' for i in range(1, num_samples + 1)]\n",
    "post_out_ids = ['POID' + f'{i:08}' for i in range(1, num_samples + 1)]\n",
    "\n",
    "start_date = pd.to_datetime('2023-09-01')\n",
    "end_date = pd.to_datetime(datetime.now().strftime('%Y-%m-%d')) \n",
    "timestamps = pd.to_datetime(np.random.randint(start_date.value, end_date.value, num_samples,dtype='int64'), unit='ns')\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'file_id': file_ids,\n",
    "    'post_out_id': post_out_ids,\n",
    "    'timestamp': timestamps\n",
    "})\n",
    "\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'].dt.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "df.to_parquet(batch7_output_filename, index=False)\n",
    "\n",
    "print(df.head())\n",
    "print(f\"Data saved to '{batch7_output_filename}'.\")\n",
    "print(df.dtypes)\n",
    "print('------------------------')\n",
    "\n",
    "\n",
    "\n",
    "### Create Batch 24\n",
    "# Set conditions\n",
    "batch24_output_filename = filepath + 'ppa_postoutbulkrequests.parquet'\n",
    "post_out_types_list = ['Prize Warrant', 'daily_files', 'Scheduled Statements']\n",
    "\n",
    "length_min_days = 1\n",
    "length_max_days = 14\n",
    "\n",
    "\n",
    "df['post_out_type'] = np.random.choice(post_out_types_list, size=len(df))\n",
    "\n",
    "random_days = pd.to_timedelta(np.random.randint(length_min_days, length_max_days, size=len(df)), unit='D')\n",
    "df['file_received_timestamp'] = df['timestamp'] - random_days\n",
    "df['file_received_timestamp'] = pd.to_datetime(df['file_received_timestamp'])\n",
    "\n",
    "df = df.drop(columns=['timestamp','post_out_id'])\n",
    "\n",
    "df.to_parquet(batch24_output_filename, index=False)\n",
    "\n",
    "print(df.head())\n",
    "print(f\"Data saved to '{batch24_output_filename}'.\")\n",
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Mock Data\n",
    "\n",
    "Daily Batch Feed 8 servicenow_firstcontactresolution\n",
    "\n",
    "Includes the following columns:\n",
    "- crm_id\n",
    "- created_timestamp\n",
    "- status_timestamp\n",
    "- status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         crm_id   created_timestamp    status_timestamp  \\\n",
      "0  CUST00000001 2023-11-13 18:57:55 2023-11-15 09:57:55   \n",
      "1  CUST00000002 2023-09-19 14:52:21 2023-09-21 05:52:21   \n",
      "2  CUST00000003 2024-02-12 20:30:53 2024-02-14 11:30:53   \n",
      "3  CUST00000004 2024-01-13 07:09:57 2024-01-14 22:09:57   \n",
      "4  CUST00000005 2024-01-30 11:04:39 2024-02-01 02:04:39   \n",
      "\n",
      "                      status  \n",
      "0  Passed to case management  \n",
      "1  Passed to case management  \n",
      "2  Passed to case management  \n",
      "3                  Completed  \n",
      "4                  Completed  \n",
      "Data saved to 'C:/Projects/NS&I/new_mockfiles/servicenow_firstcontactresolution.parquet'.\n",
      "crm_id                       object\n",
      "created_timestamp    datetime64[ns]\n",
      "status_timestamp     datetime64[ns]\n",
      "status                       object\n",
      "dtype: object\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "# Mock data for Batch #8 - servicenow_firstcontactresolution\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "### Create Batch 8\n",
    "# Set conditions\n",
    "np.random.seed(0)\n",
    "filepath = 'C:/Projects/NS&I/new_mockfiles/'\n",
    "batch8_output_filename = filepath + 'servicenow_firstcontactresolution.parquet'\n",
    "num_samples = 3000\n",
    "\n",
    "status_list = ['Pending','Passed to case management','Completed']\n",
    "\n",
    "\n",
    "crm_id = ['CUST' + f'{i:08}' for i in range(1, num_samples + 1)]\n",
    "\n",
    "start_date = pd.to_datetime('2023-09-01')\n",
    "end_date = pd.to_datetime(datetime.now().strftime('%Y-%m-%d')) \n",
    "created_timestamp = pd.to_datetime(np.random.randint(start_date.value, end_date.value, num_samples,dtype='int64'), unit='ns')\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'crm_id': crm_id,\n",
    "    'created_timestamp': created_timestamp\n",
    "})\n",
    "\n",
    "df['created_timestamp'] = pd.to_datetime(df['created_timestamp']).dt.round('s')\n",
    "df['status_timestamp'] = pd.to_datetime(df['created_timestamp']).dt.round('s') + pd.Timedelta(hours=np.random.randint(20,50))\n",
    "\n",
    "df['status'] = np.random.choice(status_list, size=len(df))\n",
    "\n",
    "df\n",
    "\n",
    "df.to_parquet(batch8_output_filename, engine='pyarrow', index=False)\n",
    "\n",
    "print(df.head())\n",
    "print(f\"Data saved to '{batch8_output_filename}'.\")\n",
    "print(df.dtypes)\n",
    "print('------------------------')\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Mock Data\n",
    "\n",
    "Daily Batch Feed 9 - sps_chequereceived,10 - sps_chequeprocessed,11 - sps_chequedestroyed \n",
    "\n",
    "9 Includes the following columns:\n",
    "- Cheque_id\t\n",
    "- received timestamp\t\n",
    "- valid status\t\n",
    "- type\n",
    "\n",
    "10 Includes the following columns:\n",
    "- cheque_id\t\n",
    "- processed_timestamp\n",
    "\n",
    "11 Includes the following columns:\n",
    "- cheque_id\t\n",
    "- destroyed_timestamp\t\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   cheque_id  received_timestamp valid_status     type\n",
      "0   10000052 2023-11-13 18:57:55      Invalid  warrant\n",
      "1   10001914 2023-09-19 14:52:21      Invalid  warrant\n",
      "2   10002622 2024-02-12 20:30:53      Invalid   cheque\n",
      "3   10001938 2024-01-13 07:09:57        Valid   cheque\n",
      "4   10001859 2024-01-30 11:04:39      Invalid  warrant\n",
      "Data saved to 'C:/Projects/NS&I/new_mockfiles/sps_chequereceived.parquet'.\n",
      "cheque_id                      int32\n",
      "received_timestamp    datetime64[ns]\n",
      "valid_status                  object\n",
      "type                          object\n",
      "dtype: object\n",
      "------------------------\n",
      "   cheque_id processed_timestamp\n",
      "0   10000052 2023-11-14 15:57:55\n",
      "1   10001914 2023-09-20 11:52:21\n",
      "2   10002622 2024-02-13 17:30:53\n",
      "3   10001938 2024-01-14 04:09:57\n",
      "4   10001859 2024-01-31 08:04:39\n",
      "Data saved to 'C:/Projects/NS&I/new_mockfiles/sps_chequeprocessed.parquet'.\n",
      "cheque_id                       int32\n",
      "processed_timestamp    datetime64[ns]\n",
      "dtype: object\n",
      "------------------------\n",
      "   cheque_id destroyed_timestamp\n",
      "0   10000052 2023-12-02 19:57:55\n",
      "1   10001914 2023-10-08 15:52:21\n",
      "2   10002622 2024-03-02 21:30:53\n",
      "3   10001938 2024-02-01 08:09:57\n",
      "4   10001859 2024-02-18 12:04:39\n",
      "Data saved to 'C:/Projects/NS&I/new_mockfiles/sps_chequedestoyed.parquet'.\n",
      "cheque_id                       int32\n",
      "destroyed_timestamp    datetime64[ns]\n",
      "dtype: object\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "# Mock data for Batch #9 - sps_chequereceived\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "### Create Batch 9\n",
    "# Set conditions\n",
    "np.random.seed(0)\n",
    "filepath = 'C:/Projects/NS&I/new_mockfiles/'\n",
    "\n",
    "batch9_output_filename = filepath + 'sps_chequereceived.parquet'\n",
    "num_samples = 3000\n",
    "\n",
    "valid_status_list = ['Valid','Invalid']\n",
    "type_list = ['cheque','warrant']\n",
    "\n",
    "\n",
    "start_date = pd.to_datetime('2023-09-01')\n",
    "end_date = pd.to_datetime(datetime.now().strftime('%Y-%m-%d')) \n",
    "received_timestamp = pd.to_datetime(np.random.randint(start_date.value, end_date.value, num_samples,dtype='int64'), unit='ns')\n",
    "cheque_id = np.random.randint(10000001, 10003000, num_samples)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'cheque_id' : cheque_id,\n",
    "    'received_timestamp': received_timestamp\n",
    "})\n",
    "\n",
    "df['valid_status'] = np.random.choice(valid_status_list, size=len(df))\n",
    "df['type'] = np.random.choice(type_list, size=len(df))\n",
    "df['received_timestamp'] = pd.to_datetime(df['received_timestamp']).dt.round('s')\n",
    "\n",
    "df\n",
    "\n",
    "df.to_parquet(batch9_output_filename, engine='pyarrow', index=False)\n",
    "\n",
    "print(df.head())\n",
    "print(f\"Data saved to '{batch9_output_filename}'.\")\n",
    "print(df.dtypes)\n",
    "print('------------------------')\n",
    "\n",
    "### Create Batch 10\n",
    "batch10_output_filename = filepath + 'sps_chequeprocessed.parquet'\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'cheque_id' : cheque_id,\n",
    "    'processed_timestamp': received_timestamp\n",
    "})\n",
    "\n",
    "df['processed_timestamp'] = pd.to_datetime(df['processed_timestamp']).dt.round('s') + pd.Timedelta(hours=np.random.randint(5,24))\n",
    "\n",
    "df\n",
    "\n",
    "df.to_parquet(batch10_output_filename, engine='pyarrow', index=False)\n",
    "\n",
    "print(df.head())\n",
    "print(f\"Data saved to '{batch10_output_filename}'.\")\n",
    "print(df.dtypes)\n",
    "print('------------------------')\n",
    "\n",
    "\n",
    "### Create Batch 11\n",
    "batch11_output_filename = filepath + 'sps_chequedestoyed.parquet'\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'cheque_id' : cheque_id,\n",
    "    'destroyed_timestamp': received_timestamp\n",
    "})\n",
    "\n",
    "df['destroyed_timestamp'] = pd.to_datetime(df['destroyed_timestamp']).dt.round('s') + pd.Timedelta(hours=np.random.randint(48,480))\n",
    "\n",
    "df\n",
    "\n",
    "df.to_parquet(batch11_output_filename, engine='pyarrow', index=False)\n",
    "\n",
    "print(df.head())\n",
    "print(f\"Data saved to '{batch11_output_filename}'.\")\n",
    "print(df.dtypes)\n",
    "print('------------------------')\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Mock Data\n",
    "\n",
    "Daily Batch Feed 12 - ssl_training,15 - ssl_training_schedule \n",
    "\n",
    "12 Includes the following columns:\n",
    "- employee_id\t\n",
    "- Status\t\n",
    "- training_id\t\n",
    "- training_completion_date\n",
    "\n",
    "15 Includes the following columns:\n",
    "- training_id\n",
    "- training_course\n",
    "- schedule_date\t\n",
    "- training_due_date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   employee_id  training_id training_completion_date       status\n",
      "0  EMP00000001  TRN00000001      2023-11-13 18:57:55  In-Progress\n",
      "1  EMP00000002  TRN00000002      2023-09-19 14:52:21  In-Progress\n",
      "2  EMP00000003  TRN00000003      2024-02-12 20:30:53  In-Progress\n",
      "3  EMP00000004  TRN00000004      2024-01-13 07:09:57     Complete\n",
      "4  EMP00000005  TRN00000005      2024-01-30 11:04:39     Complete\n",
      "Data saved to 'C:/Projects/NS&I/new_mockfiles/ssl_training.parquet'.\n",
      "employee_id                         object\n",
      "training_id                         object\n",
      "training_completion_date    datetime64[ns]\n",
      "status                              object\n",
      "dtype: object\n",
      "------------------------\n",
      "   training_id       schedule_date training_course   training_due_date\n",
      "0  TRN00000001 2023-12-19 18:45:42     Soft Skills 2024-01-18 18:45:42\n",
      "1  TRN00000002 2024-03-20 20:42:16     Soft Skills 2024-04-19 20:42:16\n",
      "2  TRN00000003 2024-03-07 08:07:03            ITSM 2024-04-06 08:07:03\n",
      "3  TRN00000004 2023-11-20 20:32:44     Soft Skills 2023-12-20 20:32:44\n",
      "4  TRN00000005 2024-04-07 23:03:29     Soft Skills 2024-05-07 23:03:29\n",
      "Data saved to 'C:/Projects/NS&I/new_mockfiles/ssl_trainingschedule.parquet'.\n",
      "training_id                  object\n",
      "schedule_date        datetime64[ns]\n",
      "training_course              object\n",
      "training_due_date    datetime64[ns]\n",
      "dtype: object\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "# Mock data for Batch #12 - SSL Training\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "### Create Batch 12\n",
    "# Set conditions\n",
    "np.random.seed(0)\n",
    "filepath = 'C:/Projects/NS&I/new_mockfiles/'\n",
    "batch12_output_filename = filepath + 'ssl_training.parquet'\n",
    "num_samples = 3000\n",
    "\n",
    "\n",
    "status_list = ['Not Started','In-Progress','Complete']\n",
    "training_course_list = ['']\n",
    "\n",
    "employee_id = ['EMP' + f'{i:08}' for i in range(1, num_samples + 1)]\n",
    "training_id = ['TRN' + f'{i:08}' for i in range(1, num_samples + 1)]\n",
    "\n",
    "start_date = pd.to_datetime('2023-09-01')\n",
    "end_date = pd.to_datetime(datetime.now().strftime('%Y-%m-%d')) \n",
    "training_completion_date = pd.to_datetime(np.random.randint(start_date.value, end_date.value, num_samples,dtype='int64'), unit='ns')\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'employee_id': employee_id,\n",
    "    'training_id': training_id,\n",
    "    'training_completion_date': training_completion_date\n",
    "})\n",
    "\n",
    "df['status'] = np.random.choice(status_list, size=len(df))\n",
    "df['training_completion_date'] = pd.to_datetime(df['training_completion_date']).dt.round('s')\n",
    "\n",
    "df\n",
    "\n",
    "df.to_parquet(batch12_output_filename, engine='pyarrow', index=False)\n",
    "\n",
    "print(df.head())\n",
    "print(f\"Data saved to '{batch12_output_filename}'.\")\n",
    "print(df.dtypes)\n",
    "print('------------------------')\n",
    "\n",
    "\n",
    "### Create Batch 15\n",
    "# Set conditions\n",
    "batch15_output_filename = filepath + 'ssl_trainingschedule.parquet'\n",
    "\n",
    "\n",
    "training_course_list = ['Soft Skills','ITSM']\n",
    "\n",
    "training_id = ['TRN' + f'{i:08}' for i in range(1, num_samples + 1)]\n",
    "\n",
    "start_date = pd.to_datetime('2023-09-01')\n",
    "end_date = pd.to_datetime(datetime.now().strftime('%Y-%m-%d')) \n",
    "schedule_date = pd.to_datetime(np.random.randint(start_date.value, end_date.value, num_samples,dtype='int64'), unit='ns')\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'training_id': training_id,\n",
    "    'schedule_date': schedule_date\n",
    "})\n",
    "\n",
    "df['training_course'] = np.random.choice(training_course_list, size=len(df))\n",
    "df['schedule_date'] = pd.to_datetime(df['schedule_date']).dt.round('s')\n",
    "df['training_due_date'] = pd.to_datetime(df['schedule_date']) + pd.Timedelta(hours=720)\n",
    "\n",
    "\n",
    "df\n",
    "\n",
    "df.to_parquet(batch15_output_filename, engine='pyarrow', index=False)\n",
    "\n",
    "print(df.head())\n",
    "print(f\"Data saved to '{batch15_output_filename}'.\")\n",
    "print(df.dtypes)\n",
    "print('------------------------')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Mock Data\n",
    "\n",
    "Daily Batch Feed 13 - ssl_employee_base,14 - ssl_employee_jl \n",
    "\n",
    "13 Includes the following columns:\n",
    "- employee_id\t\n",
    "- account_join_date\n",
    "\n",
    "14 Includes the following columns:\n",
    "- employee_id\t\n",
    "- account_join_date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   employee_id   account_join_date\n",
      "0  EMP00000001 2023-11-13 18:57:55\n",
      "1  EMP00000002 2023-09-19 14:52:21\n",
      "2  EMP00000003 2024-02-12 20:30:53\n",
      "3  EMP00000004 2024-01-13 07:09:57\n",
      "4  EMP00000005 2024-01-30 11:04:39\n",
      "Data saved to 'C:/Projects/NS&I/new_mockfiles/ssl_employee_base.parquet'.\n",
      "employee_id                  object\n",
      "account_join_date    datetime64[ns]\n",
      "dtype: object\n",
      "------------------------\n",
      "   employee_id   account_join_date\n",
      "0  EMP00000001 2023-11-13 18:57:55\n",
      "1  EMP00000002 2023-09-19 14:52:21\n",
      "2  EMP00000003 2024-02-12 20:30:53\n",
      "3  EMP00000004 2024-01-13 07:09:57\n",
      "4  EMP00000005 2024-01-30 11:04:39\n",
      "Data saved to 'C:/Projects/NS&I/new_mockfiles/ssl_trainingschedule.parquet'.\n",
      "employee_id                  object\n",
      "account_join_date    datetime64[ns]\n",
      "dtype: object\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "# Mock data for Batch #12 - SSL Training\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "### Create Batch 13\n",
    "# Set conditions\n",
    "np.random.seed(0)\n",
    "filepath = 'C:/Projects/NS&I/new_mockfiles/'\n",
    "batch13_output_filename = filepath + 'ssl_employee_base.parquet'\n",
    "num_samples = 3000\n",
    "\n",
    "\n",
    "\n",
    "employee_id = ['EMP' + f'{i:08}' for i in range(1, num_samples + 1)]\n",
    "\n",
    "start_date = pd.to_datetime('2023-09-01')\n",
    "end_date = pd.to_datetime(datetime.now().strftime('%Y-%m-%d')) \n",
    "account_join_date = pd.to_datetime(np.random.randint(start_date.value, end_date.value, num_samples,dtype='int64'), unit='ns')\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'employee_id': employee_id,\n",
    "    'account_join_date': training_completion_date\n",
    "})\n",
    "\n",
    "\n",
    "df['account_join_date'] = pd.to_datetime(df['account_join_date']).dt.round('s')\n",
    "\n",
    "df\n",
    "\n",
    "df.to_parquet(batch13_output_filename, engine='pyarrow', index=False)\n",
    "\n",
    "print(df.head())\n",
    "print(f\"Data saved to '{batch13_output_filename}'.\")\n",
    "print(df.dtypes)\n",
    "print('------------------------')\n",
    "\n",
    "\n",
    "### Create Batch 14\n",
    "# Set conditions\n",
    "batch14_output_filename = filepath + 'ssl_employee_jl.parquet'\n",
    "\n",
    "employee_id = ['EMP' + f'{i:08}' for i in range(1, num_samples + 1)]\n",
    "\n",
    "start_date = pd.to_datetime('2023-09-01')\n",
    "end_date = pd.to_datetime(datetime.now().strftime('%Y-%m-%d')) \n",
    "account_join_date = pd.to_datetime(np.random.randint(start_date.value, end_date.value, num_samples,dtype='int64'), unit='ns')\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'employee_id': employee_id,\n",
    "    'account_join_date': training_completion_date\n",
    "})\n",
    "\n",
    "\n",
    "df['account_join_date'] = pd.to_datetime(df['account_join_date']).dt.round('s')\n",
    "\n",
    "df\n",
    "\n",
    "df.to_parquet(batch14_output_filename, engine='pyarrow', index=False)\n",
    "\n",
    "print(df.head())\n",
    "print(f\"Data saved to '{batch15_output_filename}'.\")\n",
    "print(df.dtypes)\n",
    "print('------------------------')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Mock Data\n",
    "\n",
    "Daily Batch Feed 16 servicenow_bonding\n",
    "\n",
    "16 Includes the following columns:\n",
    "- priority\n",
    "- communication_time\n",
    "- created_timestamp\n",
    "- incident_id\n",
    "- bonded_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   incident_id  communication_time   created_timestamp  priority  \\\n",
      "0  INC00000001                  53 2023-11-13 18:57:55         2   \n",
      "1  INC00000002                  30 2023-09-19 14:52:21         2   \n",
      "2  INC00000003                  39 2024-02-12 20:30:53         1   \n",
      "3  INC00000004                  54 2024-01-13 07:09:57         2   \n",
      "4  INC00000005                  12 2024-01-30 11:04:39         1   \n",
      "\n",
      "     bonded_timestamp  \n",
      "0 2023-11-14 09:57:55  \n",
      "1 2023-09-20 05:52:21  \n",
      "2 2024-02-13 11:30:53  \n",
      "3 2024-01-13 22:09:57  \n",
      "4 2024-01-31 02:04:39  \n",
      "Data saved to 'C:/Projects/NS&I/new_mockfiles/servicenow_bonding.parquet'.\n",
      "incident_id                   object\n",
      "communication_time             int64\n",
      "created_timestamp     datetime64[ns]\n",
      "priority                       int32\n",
      "bonded_timestamp      datetime64[ns]\n",
      "dtype: object\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "# Mock data for Batch #16 - SSL Training\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "### Create Batch 16\n",
    "# Set conditions\n",
    "np.random.seed(0)\n",
    "filepath = 'C:/Projects/NS&I/new_mockfiles/'\n",
    "batch16_output_filename = filepath + 'servicenow_bonding.parquet'\n",
    "num_samples = 3000\n",
    "priority_list = [1,2]\n",
    "\n",
    "\n",
    "\n",
    "incident_id = ['INC' + f'{i:08}' for i in range(1, num_samples + 1)]\n",
    "\n",
    "start_date = pd.to_datetime('2023-09-01')\n",
    "end_date = pd.to_datetime(datetime.now().strftime('%Y-%m-%d')) \n",
    "created_timestamp = pd.to_datetime(np.random.randint(start_date.value, end_date.value, num_samples,dtype='int64'), unit='ns')\n",
    "communication_time = np.random.randint(10, 60, num_samples,dtype='int64')\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'incident_id': incident_id,\n",
    "    'communication_time': communication_time,\n",
    "    'created_timestamp': created_timestamp\n",
    "})\n",
    "\n",
    "df['priority'] = np.random.choice(priority_list, size=len(df))\n",
    "df['created_timestamp'] = pd.to_datetime(df['created_timestamp']).dt.round('s')\n",
    "df['bonded_timestamp'] = pd.to_datetime(df['created_timestamp']).dt.round('s') + pd.Timedelta(hours=np.random.randint(12,36))\n",
    "\n",
    "df\n",
    "\n",
    "df.to_parquet(batch16_output_filename, engine='pyarrow', index=False)\n",
    "\n",
    "print(df.head())\n",
    "print(f\"Data saved to '{batch16_output_filename}'.\")\n",
    "print(df.dtypes)\n",
    "print('------------------------')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Mock Data\n",
    "\n",
    "Daily Batch Feed 17 servicenow_applicationrejection\n",
    "\n",
    "17 Includes the following columns:\n",
    "- rejection_id\n",
    "- case_id\n",
    "- contact_channel\n",
    "- rejected_timestamp\n",
    "- rejection_reason\n",
    "- created_timestamp\n",
    "- total_active_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       case_id  total_active_time   created_timestamp  rejection_id  \\\n",
      "0  CAS00000001                144 2024-04-25 18:10:17             2   \n",
      "1  CAS00000002                 24 2024-04-25 23:51:48             2   \n",
      "2  CAS00000003                 96 2024-01-18 03:06:00             1   \n",
      "3  CAS00000004                 96 2024-01-26 02:41:52             1   \n",
      "4  CAS00000005                192 2023-12-15 03:40:45             1   \n",
      "\n",
      "      rejection_reason contact_channel    bonded_timestamp  \n",
      "0                Other           Phone 2024-05-01 18:10:17  \n",
      "1  unavailable_product           Phone 2024-04-26 23:51:48  \n",
      "2  unavailable_product           Email 2024-01-22 03:06:00  \n",
      "3                Other           Phone 2024-01-30 02:41:52  \n",
      "4            Not Valid           Phone 2023-12-23 03:40:45  \n",
      "Data saved to 'C:/Projects/NS&I/new_mockfiles/servicenow_applicationrejection.parquet'.\n",
      "case_id                      object\n",
      "total_active_time             int32\n",
      "created_timestamp    datetime64[ns]\n",
      "rejection_id                  int32\n",
      "rejection_reason             object\n",
      "contact_channel              object\n",
      "bonded_timestamp     datetime64[ns]\n",
      "dtype: object\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "# Mock data for Batch #17\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "### Create Batch 17\n",
    "# Set conditions\n",
    "np.random.seed(0)\n",
    "filepath = 'C:/Projects/NS&I/new_mockfiles/'\n",
    "batch17_output_filename = filepath + 'servicenow_applicationrejection.parquet'\n",
    "num_samples = 3000\n",
    "\n",
    "length_min_days=1\n",
    "length_max_days=10\n",
    "random_days = pd.to_timedelta(np.random.randint(length_min_days, length_max_days, size=len(df)), unit='D')\n",
    "\n",
    "\n",
    "rejection_id_list = [1,2,3,4]\n",
    "rejection_reason_list = ['unavailable_product','Not Valid','unacceptable_cheque','Other']\n",
    "contact_channel_list = ['Email','Phone','Chat']\n",
    "\n",
    "case_id = ['CAS' + f'{i:08}' for i in range(1, num_samples + 1)]\n",
    "\n",
    "start_date = pd.to_datetime('2023-09-01')\n",
    "end_date = pd.to_datetime(datetime.now().strftime('%Y-%m-%d')) \n",
    "created_timestamp = pd.to_datetime(np.random.randint(start_date.value, end_date.value, num_samples,dtype='int64'), unit='ns')\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'case_id': case_id,\n",
    "    'total_active_time': communication_time,\n",
    "    'created_timestamp': created_timestamp\n",
    "})\n",
    "\n",
    "df['rejection_id'] = np.random.choice(rejection_id_list, size=len(df))\n",
    "df['rejection_reason'] = np.random.choice(rejection_reason_list, size=len(df))\n",
    "df['contact_channel'] = np.random.choice(contact_channel_list, size=len(df))\n",
    "df['created_timestamp'] = pd.to_datetime(df['created_timestamp']).dt.round('s')\n",
    "df['bonded_timestamp'] = pd.to_datetime(df['created_timestamp']).dt.round('s') + random_days\n",
    "df['total_active_time'] = df['bonded_timestamp'] -  df['created_timestamp']\n",
    "df['total_active_time'] = (df['total_active_time']*24).dt.days.astype('int32')\n",
    "\n",
    "df\n",
    "\n",
    "df.to_parquet(batch17_output_filename, engine='pyarrow', index=False)\n",
    "\n",
    "print(df.head())\n",
    "print(f\"Data saved to '{batch17_output_filename}'.\")\n",
    "print(df.dtypes)\n",
    "print('------------------------')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Mock Data\n",
    "\n",
    "Daily Batch Feed 18 servicenow_applicationcancellation\n",
    "\n",
    "18 Includes the following columns:\n",
    "- rejection_id\n",
    "- case_id\n",
    "- contact_channel\n",
    "- cancelled_timestamp\n",
    "- rejection_reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       case_id cancelled_timestamp  rejection_id rejection_reason  \\\n",
      "0  CAS00000001 2024-04-25 18:10:17             2        Cancelled   \n",
      "1  CAS00000002 2024-04-25 23:51:48             2        Cancelled   \n",
      "2  CAS00000003 2024-01-18 03:06:00             1        Cancelled   \n",
      "3  CAS00000004 2024-01-26 02:41:52             1        Cancelled   \n",
      "4  CAS00000005 2023-12-15 03:40:45             1        Cancelled   \n",
      "\n",
      "  contact_channel  \n",
      "0           Email  \n",
      "1           Email  \n",
      "2           Phone  \n",
      "3           Phone  \n",
      "4           Email  \n",
      "Data saved to 'C:/Projects/NS&I/new_mockfiles/servicenow_applicationcancellation.parquet'.\n",
      "case_id                        object\n",
      "cancelled_timestamp    datetime64[ns]\n",
      "rejection_id                    int32\n",
      "rejection_reason               object\n",
      "contact_channel                object\n",
      "dtype: object\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "# Mock data for Batch #18\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "### Create Batch 18\n",
    "# Set conditions\n",
    "np.random.seed(0)\n",
    "filepath = 'C:/Projects/NS&I/new_mockfiles/'\n",
    "batch18_output_filename = filepath + 'servicenow_applicationcancellation.parquet'\n",
    "num_samples = 3000\n",
    "\n",
    "length_min_days=1\n",
    "length_max_days=10\n",
    "random_days = pd.to_timedelta(np.random.randint(length_min_days, length_max_days, size=len(df)), unit='D')\n",
    "\n",
    "\n",
    "rejection_id_list = [1,2,3,4]\n",
    "rejection_reason_list = ['Cancelled']\n",
    "contact_channel_list = ['Email','Phone','Chat']\n",
    "\n",
    "case_id = ['CAS' + f'{i:08}' for i in range(1, num_samples + 1)]\n",
    "\n",
    "start_date = pd.to_datetime('2023-09-01')\n",
    "end_date = pd.to_datetime(datetime.now().strftime('%Y-%m-%d')) \n",
    "cancelled_timestamp = pd.to_datetime(np.random.randint(start_date.value, end_date.value, num_samples,dtype='int64'), unit='ns')\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'case_id': case_id,\n",
    "    'cancelled_timestamp': created_timestamp\n",
    "})\n",
    "\n",
    "df['rejection_id'] = np.random.choice(rejection_id_list, size=len(df))\n",
    "df['rejection_reason'] = 'Cancelled'\n",
    "df['contact_channel'] = np.random.choice(contact_channel_list, size=len(df))\n",
    "df['cancelled_timestamp'] = pd.to_datetime(df['cancelled_timestamp']).dt.round('s')\n",
    "\n",
    "df\n",
    "\n",
    "df.to_parquet(batch18_output_filename, engine='pyarrow', index=False)\n",
    "\n",
    "print(df.head())\n",
    "print(f\"Data saved to '{batch18_output_filename}'.\")\n",
    "print(df.dtypes)\n",
    "print('------------------------')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Mock Data\n",
    "\n",
    "Daily Batch Feed 19 ppa_cancellationnotification\n",
    "\n",
    "19 Includes the following columns:\n",
    "- case_id\n",
    "- notification_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       case_id notification_timestamp\n",
      "0  CAS00000001    2024-04-26 18:10:17\n",
      "1  CAS00000002    2024-04-27 23:51:48\n",
      "2  CAS00000003    2024-01-20 03:06:00\n",
      "3  CAS00000004    2024-01-27 02:41:52\n",
      "4  CAS00000005    2023-12-17 03:40:45\n",
      "Data saved to 'C:/Projects/NS&I/new_mockfiles/ppa_cancellationnotification.parquet'.\n",
      "case_id                           object\n",
      "notification_timestamp    datetime64[ns]\n",
      "dtype: object\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "# Mock data for Batch #19\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "### Create Batch 18\n",
    "# Set conditions\n",
    "np.random.seed(0)\n",
    "filepath = 'C:/Projects/NS&I/new_mockfiles/'\n",
    "batch19_output_filename = filepath + 'ppa_cancellationnotification.parquet'\n",
    "num_samples = 3000\n",
    "\n",
    "length_min_days=1\n",
    "length_max_days=3\n",
    "random_days = pd.to_timedelta(np.random.randint(length_min_days, length_max_days, size=len(df)), unit='D')\n",
    "\n",
    "\n",
    "case_id = ['CAS' + f'{i:08}' for i in range(1, num_samples + 1)]\n",
    "\n",
    "start_date = pd.to_datetime('2023-09-01')\n",
    "end_date = pd.to_datetime(datetime.now().strftime('%Y-%m-%d')) \n",
    "\n",
    "#notification_timestamp = pd.to_datetime(np.random.randint(start_date.value, end_date.value, num_samples,dtype='int64'), unit='ns')\n",
    "notification_timestamp = df['cancelled_timestamp'] # from data feed 18\n",
    "df = pd.DataFrame({\n",
    "    'case_id': case_id,\n",
    "    'notification_timestamp': notification_timestamp\n",
    "})\n",
    "\n",
    "df['notification_timestamp'] = pd.to_datetime(df['notification_timestamp']).dt.round('s') + random_days\n",
    "\n",
    "df\n",
    "\n",
    "df.to_parquet(batch19_output_filename, engine='pyarrow', index=False)\n",
    "\n",
    "print(df.head())\n",
    "print(f\"Data saved to '{batch19_output_filename}'.\")\n",
    "print(df.dtypes)\n",
    "print('------------------------')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Mock Data\n",
    "\n",
    "Daily Batch Feed 20 servicenow_caseclosed\n",
    "\n",
    "20 Includes the following columns:\n",
    "- fos_flag\n",
    "- closed_timestamp\n",
    "- case_id\n",
    "- inbound_channel\n",
    "- time_to_action\n",
    "- case_type\n",
    "- created_timestamp\n",
    "- extension_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       case_id   created_timestamp fos_flag extension_flag Inbound_channel  \\\n",
      "0  CAS00000001 2024-04-25 18:10:17        N              N            Post   \n",
      "1  CAS00000002 2024-04-25 23:51:48        N              Y            Post   \n",
      "2  CAS00000003 2024-01-18 03:06:00        Y              Y            Post   \n",
      "3  CAS00000004 2024-01-26 02:41:52        Y              N            Post   \n",
      "4  CAS00000005 2023-12-15 03:40:45        Y              N            Post   \n",
      "\n",
      "     closed_timestamp  time_to_action  \n",
      "0 2024-05-01 18:10:17               6  \n",
      "1 2024-04-26 23:51:48               1  \n",
      "2 2024-01-22 03:06:00               4  \n",
      "3 2024-01-30 02:41:52               4  \n",
      "4 2023-12-23 03:40:45               8  \n",
      "Data saved to 'C:/Projects/NS&I/new_mockfiles/servicenow_caseclosed.parquet'.\n",
      "case_id                      object\n",
      "created_timestamp    datetime64[ns]\n",
      "fos_flag                     object\n",
      "extension_flag               object\n",
      "Inbound_channel              object\n",
      "closed_timestamp     datetime64[ns]\n",
      "time_to_action                int32\n",
      "dtype: object\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "# Mock data for Batch #20\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "### Create Batch 20\n",
    "# Set conditions\n",
    "np.random.seed(0)\n",
    "filepath = 'C:/Projects/NS&I/new_mockfiles/'\n",
    "batch20_output_filename = filepath + 'servicenow_caseclosed.parquet'\n",
    "num_samples = 3000\n",
    "\n",
    "length_min_days=1\n",
    "length_max_days=10\n",
    "random_days = pd.to_timedelta(np.random.randint(length_min_days, length_max_days, size=len(df)), unit='D')\n",
    "\n",
    "\n",
    "case_type_list = ['Change_of_Detail','Subject_Access_Request','Sales_Application','Account_Trace',\n",
    "                         'Account_Query','Complaint','Telephony_Registration','Online_Registration','Bereavement']\n",
    "fos_flag_list = ['Y','N']\n",
    "extension_flag_list = ['Y','N']\n",
    "\n",
    "case_id = ['CAS' + f'{i:08}' for i in range(1, num_samples + 1)]\n",
    "\n",
    "start_date = pd.to_datetime('2023-09-01')\n",
    "end_date = pd.to_datetime(datetime.now().strftime('%Y-%m-%d')) \n",
    "created_timestamp = pd.to_datetime(np.random.randint(start_date.value, end_date.value, num_samples,dtype='int64'), unit='ns')\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'case_id': case_id,\n",
    "    'created_timestamp': created_timestamp\n",
    "})\n",
    "\n",
    "df['fos_flag'] = np.random.choice(fos_flag_list, size=len(df))\n",
    "df['extension_flag'] = np.random.choice(extension_flag_list, size=len(df))\n",
    "df['Inbound_channel'] = 'Post'\n",
    "df['created_timestamp'] = pd.to_datetime(df['created_timestamp']).dt.round('s')\n",
    "df['closed_timestamp'] = pd.to_datetime(df['created_timestamp']).dt.round('s') + random_days\n",
    "df['time_to_action'] = df['closed_timestamp'] -  df['created_timestamp']\n",
    "df['time_to_action'] = df['time_to_action'].dt.days.astype('int32')\n",
    "\n",
    "df\n",
    "\n",
    "df.to_parquet(batch20_output_filename, engine='pyarrow', index=False)\n",
    "\n",
    "print(df.head())\n",
    "print(f\"Data saved to '{batch20_output_filename}'.\")\n",
    "print(df.dtypes)\n",
    "print('------------------------')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Mock Data\n",
    "\n",
    "Daily Batch Feed 21 servicenow_responded\n",
    "\n",
    "21 Includes the following columns:\n",
    "- requested_further_information_timestamp\n",
    "- case_id\n",
    "- time_to_action\n",
    "- case_type\n",
    "- created_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       case_id   created_timestamp requested_further_information_timestamp  \\\n",
      "0  CAS00000001 2024-04-26 18:10:17                     2024-05-02 18:10:17   \n",
      "1  CAS00000002 2024-04-26 23:51:48                     2024-04-27 23:51:48   \n",
      "2  CAS00000003 2024-01-19 03:06:00                     2024-01-23 03:06:00   \n",
      "3  CAS00000004 2024-01-27 02:41:52                     2024-01-31 02:41:52   \n",
      "4  CAS00000005 2023-12-16 03:40:45                     2023-12-24 03:40:45   \n",
      "\n",
      "   time_to_action  \n",
      "0               6  \n",
      "1               1  \n",
      "2               4  \n",
      "3               4  \n",
      "4               8  \n",
      "Data saved to 'C:/Projects/NS&I/new_mockfiles/servicenow_responded.parquet'.\n",
      "case_id                                            object\n",
      "created_timestamp                          datetime64[ns]\n",
      "requested_further_information_timestamp    datetime64[ns]\n",
      "time_to_action                                      int32\n",
      "dtype: object\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "# Mock data for Batch #21\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "### Create Batch 21\n",
    "# Set conditions\n",
    "np.random.seed(0)\n",
    "filepath = 'C:/Projects/NS&I/new_mockfiles/'\n",
    "batch21_output_filename = filepath + 'servicenow_responded.parquet'\n",
    "num_samples = 3000\n",
    "\n",
    "length_min_days=1\n",
    "length_max_days=10\n",
    "random_days = pd.to_timedelta(np.random.randint(length_min_days, length_max_days, size=len(df)), unit='D')\n",
    "\n",
    "\n",
    "case_type_list = ['Change_of_Detail','Subject_Access_Request','Sales_Application','Account_Trace',\n",
    "                         'Account_Query','Complaint','Telephony_Registration','Online_Registration','Bereavement']\n",
    "\n",
    "case_id = ['CAS' + f'{i:08}' for i in range(1, num_samples + 1)]\n",
    "\n",
    "start_date = pd.to_datetime('2023-09-01')\n",
    "end_date = pd.to_datetime(datetime.now().strftime('%Y-%m-%d')) \n",
    "created_timestamp = df['created_timestamp'] + pd.Timedelta(days=1) # from datafeed 20\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'case_id': case_id,\n",
    "    'created_timestamp': created_timestamp\n",
    "})\n",
    "\n",
    "df['created_timestamp'] = pd.to_datetime(df['created_timestamp']).dt.round('s')\n",
    "df['requested_further_information_timestamp'] = pd.to_datetime(df['created_timestamp']).dt.round('s') + random_days\n",
    "df['time_to_action'] = df['requested_further_information_timestamp'] -  df['created_timestamp']\n",
    "df['time_to_action'] = df['time_to_action'].dt.days.astype('int32')\n",
    "\n",
    "df\n",
    "\n",
    "df.to_parquet(batch21_output_filename, engine='pyarrow', index=False)\n",
    "\n",
    "print(df.head())\n",
    "print(f\"Data saved to '{batch21_output_filename}'.\")\n",
    "print(df.dtypes)\n",
    "print('------------------------')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Mock Data\n",
    "\n",
    "Daily Batch Feed 22 nicecxone_messageresponses\n",
    "\n",
    "22 Includes the following columns:\n",
    "- message_type\n",
    "- number_of_messages_received\n",
    "- number_of_messages_responded_to_in_sla\n",
    "- created_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   number_of_messages_received  number_of_messages_responded_to_in_sla  \\\n",
      "0                           81                                      75   \n",
      "1                           90                                      78   \n",
      "2                           82                                      74   \n",
      "3                           72                                      59   \n",
      "4                           84                                      79   \n",
      "\n",
      "    created_timestamp  \n",
      "0 2023-11-13 18:57:55  \n",
      "1 2023-09-19 14:52:21  \n",
      "2 2024-02-12 20:30:53  \n",
      "3 2024-01-13 07:09:57  \n",
      "4 2024-01-30 11:04:39  \n",
      "Data saved to 'C:/Projects/NS&I/new_mockfiles/nicecxone_messageresponses.parquet'.\n",
      "number_of_messages_received                        int64\n",
      "number_of_messages_responded_to_in_sla             int64\n",
      "created_timestamp                         datetime64[ns]\n",
      "dtype: object\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "# Mock data for Batch #22\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "### Create Batch 22\n",
    "# Set conditions\n",
    "np.random.seed(0)\n",
    "filepath = 'C:/Projects/NS&I/new_mockfiles/'\n",
    "batch22_output_filename = filepath + 'nicecxone_messageresponses.parquet'\n",
    "num_samples = 3000\n",
    "\n",
    "message_type_list = ['Email','Twitter','Secure Message','Webchat','Facebook']\n",
    "\n",
    "start_date = pd.to_datetime('2023-09-01')\n",
    "end_date = pd.to_datetime(datetime.now().strftime('%Y-%m-%d')) \n",
    "created_timestamp = pd.to_datetime(np.random.randint(start_date.value, end_date.value, num_samples,dtype='int64'), unit='ns')\n",
    "number_of_messages_received = np.random.randint(70, 99, num_samples,dtype='int64')\n",
    "number_of_messages_responded_to_in_sla = number_of_messages_received - np.random.randint(5, 15, num_samples,dtype='int64')\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'number_of_messages_received': number_of_messages_received,\n",
    "    'number_of_messages_responded_to_in_sla': number_of_messages_responded_to_in_sla,\n",
    "    'created_timestamp': created_timestamp\n",
    "})\n",
    "\n",
    "df['created_timestamp'] = pd.to_datetime(df['created_timestamp']).dt.round('s')\n",
    "\n",
    "\n",
    "df\n",
    "\n",
    "df.to_parquet(batch22_output_filename, engine='pyarrow', index=False)\n",
    "\n",
    "print(df.head())\n",
    "print(f\"Data saved to '{batch22_output_filename}'.\")\n",
    "print(df.dtypes)\n",
    "print('------------------------')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Mock Data\n",
    "\n",
    "Daily Batch Feed 23 api_logs\n",
    "\n",
    "23 Includes the following columns:\n",
    "- api_id\n",
    "- message_received_timestamp\n",
    "- api_description\n",
    "- message_call_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        api_id  message_call_timestamp message_received_timestamp  \\\n",
      "0  API00000001 2024-03-28 02:24:17.782    2024-03-28 02:24:19.466   \n",
      "1  API00000002 2023-10-31 23:34:25.546    2023-10-31 23:34:27.105   \n",
      "2  API00000003 2023-09-22 21:11:45.583    2023-09-22 21:11:47.418   \n",
      "3  API00000004 2023-10-04 16:04:19.955    2023-10-04 16:04:21.718   \n",
      "4  API00000005 2023-12-03 05:34:57.772    2023-12-03 05:34:59.805   \n",
      "\n",
      "       api_description  \n",
      "0  User Management API  \n",
      "1  User Management API  \n",
      "2  User Management API  \n",
      "3  User Management API  \n",
      "4        Document_View  \n",
      "Data saved to 'C:/Projects/NS&I/new_mockfiles/api_logs.parquet'.\n",
      "api_id                                object\n",
      "message_call_timestamp        datetime64[ns]\n",
      "message_received_timestamp    datetime64[ns]\n",
      "api_description                       object\n",
      "dtype: object\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "# Mock data for Batch #23\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "### Create Batch 23\n",
    "# Set conditions\n",
    "np.random.seed(0)\n",
    "filepath = 'C:/Projects/NS&I/new_mockfiles/'\n",
    "batch23_output_filename = filepath + 'api_logs.parquet'\n",
    "num_samples = 3000\n",
    "\n",
    "length_min_ms=1000\n",
    "length_max_ms=2100\n",
    "random_ms = pd.to_timedelta(np.random.randint(length_min_ms, length_max_ms, size=len(df)), unit='ms')\n",
    "\n",
    "api_id = ['API' + f'{i:08}' for i in range(1, num_samples + 1)]\n",
    "api_description_list = ['Payment Gateway API','Product Catalog API','User Management API','Document_View']\n",
    "\n",
    "start_date = pd.to_datetime('2023-09-01')\n",
    "end_date = pd.to_datetime(datetime.now().strftime('%Y-%m-%d')) \n",
    "message_call_timestamp = pd.to_datetime(np.random.randint(start_date.value, end_date.value, num_samples,dtype='int64'), unit='ns')\n",
    "message_received_timestamp = message_call_timestamp + random_ms\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'api_id':api_id,\n",
    "    'message_call_timestamp': message_call_timestamp,\n",
    "    'message_received_timestamp': message_received_timestamp\n",
    "})\n",
    "\n",
    "df['api_description'] = np.random.choice(api_description_list, size=len(df))\n",
    "df['message_call_timestamp'] = pd.to_datetime(df['message_call_timestamp']).dt.round('ms')\n",
    "df['message_received_timestamp'] = pd.to_datetime(df['message_received_timestamp']).dt.round('ms')\n",
    "\n",
    "\n",
    "df\n",
    "\n",
    "df.to_parquet(batch23_output_filename, engine='pyarrow', index=False)\n",
    "\n",
    "print(df.head())\n",
    "print(f\"Data saved to '{batch23_output_filename}'.\")\n",
    "print(df.dtypes)\n",
    "print('------------------------')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Mock Data\n",
    "\n",
    "Daily Batch Feed 25 servicenow_requestforinformation\n",
    "\n",
    "25 Includes the following columns:\n",
    "- blocked_timestamp\n",
    "- request_type\n",
    "- case_id\n",
    "- case_type\n",
    "- created_timestamp\n",
    "- blocked_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       case_id   created_timestamp          case_type request_type  \\\n",
      "0  CAS00000001 2023-11-13 18:57:55     Postal Payment         ID&V   \n",
      "1  CAS00000002 2023-09-19 14:52:21  Sales_Application       Online   \n",
      "2  CAS00000003 2024-02-12 20:30:53     Postal Payment       Online   \n",
      "3  CAS00000004 2024-01-13 07:09:57  Sales_Application       Online   \n",
      "4  CAS00000005 2024-01-30 11:04:39  Sales_Application       Mobile   \n",
      "\n",
      "   blocked_sequence   blocked_timestamp  \n",
      "0                 2 2023-11-18 18:57:55  \n",
      "1                 1 2023-09-23 14:52:21  \n",
      "2                 1 2024-02-15 20:30:53  \n",
      "3                 2 2024-01-17 07:09:57  \n",
      "4                 2 2024-02-02 11:04:39  \n",
      "Data saved to 'C:/Projects/NS&I/new_mockfiles/servicenow_requestforinformation.parquet'.\n",
      "case_id                      object\n",
      "created_timestamp    datetime64[ns]\n",
      "case_type                    object\n",
      "request_type                 object\n",
      "blocked_sequence              int64\n",
      "blocked_timestamp    datetime64[ns]\n",
      "dtype: object\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "# Mock data for Batch #25 - Request For Information\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "### Create Batch 25\n",
    "# Set conditions\n",
    "np.random.seed(0)\n",
    "filepath = 'C:/Projects/NS&I/new_mockfiles/'\n",
    "batch25_output_filename = filepath + 'servicenow_requestforinformation.parquet'\n",
    "num_samples = 3000\n",
    "\n",
    "length_min_days = 1\n",
    "length_max_days = 6\n",
    "case_types_list = ['Sales_Application','Postal Payment']\n",
    "request_types_list = ['Online','Mobile','ID&V']\n",
    "blocked_sequence_list = [1,2]\n",
    "\n",
    "case_ids = ['CAS' + f'{i:08}' for i in range(1, num_samples + 1)]\n",
    "\n",
    "start_date = pd.to_datetime('2023-09-01')\n",
    "end_date = pd.to_datetime(datetime.now().strftime('%Y-%m-%d')) \n",
    "created_timestamp = pd.to_datetime(np.random.randint(start_date.value, end_date.value, num_samples,dtype='int64'), unit='ns')\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'case_id': case_ids,\n",
    "    'created_timestamp': created_timestamp\n",
    "})\n",
    "\n",
    "df['case_type'] = np.random.choice(case_types_list, size=len(df))\n",
    "df['request_type'] = np.random.choice(request_types_list, size=len(df))\n",
    "df['blocked_sequence'] = np.random.choice(blocked_sequence_list, size=len(df)).astype('int64')\n",
    "df['created_timestamp'] = pd.to_datetime(df['created_timestamp']).dt.round('s')\n",
    "\n",
    "\n",
    "random_days = pd.to_timedelta(np.random.randint(length_min_days, length_max_days, size=len(df)), unit='D')\n",
    "df['blocked_timestamp'] = df['created_timestamp'] + random_days\n",
    "df['blocked_timestamp'] = pd.to_datetime(df['blocked_timestamp']).dt.round('s')\n",
    "\n",
    "df\n",
    "\n",
    "df.to_parquet(batch25_output_filename, engine='pyarrow', index=False)\n",
    "\n",
    "print(df.head())\n",
    "print(f\"Data saved to '{batch25_output_filename}'.\")\n",
    "print(df.dtypes)\n",
    "print('------------------------')\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Mock Data\n",
    "\n",
    "Daily Batch Feed 26 ssl_audittracking\n",
    "\n",
    "26 Includes the following columns:\n",
    "- audit_id\n",
    "- resolution_due_date\n",
    "- date_raised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  audit_id         date_raised resolution_due_date\n",
      "0  AUD0001 2023-11-13 18:57:55 2023-12-13 18:57:55\n",
      "1  AUD0002 2023-09-19 14:52:21 2023-10-19 14:52:21\n",
      "2  AUD0003 2024-02-12 20:30:53 2024-03-13 20:30:53\n",
      "3  AUD0004 2024-01-13 07:09:57 2024-02-12 07:09:57\n",
      "4  AUD0005 2024-01-30 11:04:39 2024-02-29 11:04:39\n",
      "Data saved to 'C:/Projects/NS&I/new_mockfiles/ssl_audittracking.parquet'.\n",
      "audit_id                       object\n",
      "date_raised            datetime64[ns]\n",
      "resolution_due_date    datetime64[ns]\n",
      "dtype: object\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "# Mock data for Batch #26\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "### Create Batch 26\n",
    "# Set conditions\n",
    "np.random.seed(0)\n",
    "filepath = 'C:/Projects/NS&I/new_mockfiles/'\n",
    "batch26_output_filename = filepath + 'ssl_audittracking.parquet'\n",
    "num_samples = 3000\n",
    "\n",
    "length_min_days=30\n",
    "length_max_days=31\n",
    "random_days = pd.to_timedelta(np.random.randint(length_min_days, length_max_days, size=len(df)), unit='D')\n",
    "\n",
    "audit_id = ['AUD' + f'{i:04}' for i in range(1, num_samples + 1)]\n",
    "\n",
    "start_date = pd.to_datetime('2023-09-01')\n",
    "end_date = pd.to_datetime(datetime.now().strftime('%Y-%m-%d')) \n",
    "date_raised = pd.to_datetime(np.random.randint(start_date.value, end_date.value, num_samples,dtype='int64'), unit='ns')\n",
    "resolution_due_date = date_raised + random_days\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'audit_id':audit_id,\n",
    "    'date_raised': date_raised,\n",
    "    'resolution_due_date': resolution_due_date\n",
    "})\n",
    "\n",
    "df['date_raised'] = pd.to_datetime(df['date_raised']).dt.round('s')\n",
    "df['resolution_due_date'] = pd.to_datetime(df['resolution_due_date']).dt.round('s')\n",
    "\n",
    "\n",
    "df\n",
    "\n",
    "df.to_parquet(batch26_output_filename, engine='pyarrow', index=False)\n",
    "\n",
    "print(df.head())\n",
    "print(f\"Data saved to '{batch26_output_filename}'.\")\n",
    "print(df.dtypes)\n",
    "print('------------------------')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Mock Data\n",
    "\n",
    "Daily Batch Feed 27 ssl_availability\n",
    "\n",
    "27 Includes the following columns:\n",
    "- call_id\n",
    "- call_system\n",
    "- call_response\n",
    "- created_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     call_id   created_timestamp call_system call_response\n",
      "0  CALL00001 2023-11-13 18:57:55    Alfresco      Positive\n",
      "1  CALL00002 2023-09-19 14:52:21     Poly AI       Neutral\n",
      "2  CALL00003 2024-02-12 20:30:53    Alfresco       Neutral\n",
      "3  CALL00004 2024-01-13 07:09:57     Poly AI       Neutral\n",
      "4  CALL00005 2024-01-30 11:04:39     Poly AI      Negative\n",
      "Data saved to 'C:/Projects/NS&I/new_mockfiles/ssl_availability.parquet'.\n",
      "call_id                      object\n",
      "created_timestamp    datetime64[ns]\n",
      "call_system                  object\n",
      "call_response                object\n",
      "dtype: object\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "# Mock data for Batch #27\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "### Create Batch 27\n",
    "# Set conditions\n",
    "np.random.seed(0)\n",
    "filepath = 'C:/Projects/NS&I/new_mockfiles/'\n",
    "batch27_output_filename = filepath + 'ssl_availability.parquet'\n",
    "num_samples = 3000\n",
    "\n",
    "length_min_days=30\n",
    "length_max_days=31\n",
    "random_days = pd.to_timedelta(np.random.randint(length_min_days, length_max_days, size=len(df)), unit='D')\n",
    "\n",
    "call_system_list = ['Poly AI','Alfresco']\n",
    "call_response_list = ['Neutral','Negative','Positive']\n",
    "\n",
    "call_id = ['CALL' + f'{i:05}' for i in range(1, num_samples + 1)]\n",
    "\n",
    "start_date = pd.to_datetime('2023-09-01')\n",
    "end_date = pd.to_datetime(datetime.now().strftime('%Y-%m-%d')) \n",
    "created_timestamp = pd.to_datetime(np.random.randint(start_date.value, end_date.value, num_samples,dtype='int64'), unit='ns')\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'call_id':call_id,\n",
    "    'created_timestamp': created_timestamp\n",
    "})\n",
    "\n",
    "df['call_system'] = np.random.choice(call_system_list, size=len(df))\n",
    "df['call_response'] = np.random.choice(call_response_list, size=len(df))\n",
    "df['created_timestamp'] = pd.to_datetime(df['created_timestamp']).dt.round('s')\n",
    "\n",
    "\n",
    "df\n",
    "\n",
    "df.to_parquet(batch27_output_filename, engine='pyarrow', index=False)\n",
    "\n",
    "print(df.head())\n",
    "print(f\"Data saved to '{batch27_output_filename}'.\")\n",
    "print(df.dtypes)\n",
    "print('------------------------')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Mock Data\n",
    "\n",
    "Daily Batch Feed 28 servicenow_vulnerability\n",
    "\n",
    "28 Includes the following columns:\n",
    "- ticket_id\n",
    "- criticality\n",
    "- closed_timestamp\n",
    "- created_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ticket_id   created_timestamp    closed_timestamp criticality\n",
      "0  CALL00001 2023-12-06 17:31:37 2023-12-19 17:31:37    Critical\n",
      "1  CALL00002 2024-03-19 14:46:33 2024-04-04 14:46:33        High\n",
      "2  CALL00003 2024-01-31 21:08:44 2024-02-01 21:08:44        High\n",
      "3  CALL00004 2024-01-20 05:33:28 2024-01-24 05:33:28      Medium\n",
      "4  CALL00005 2023-09-16 03:45:35 2023-09-20 03:45:35         Low\n",
      "Data saved to 'C:/Projects/NS&I/new_mockfiles/servicenow_vulnerability.parquet'.\n",
      "ticket_id                    object\n",
      "created_timestamp    datetime64[ns]\n",
      "closed_timestamp     datetime64[ns]\n",
      "criticality                  object\n",
      "dtype: object\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "# Mock data for Batch #28\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "### Create Batch 28\n",
    "# Set conditions\n",
    "np.random.seed(0)\n",
    "filepath = 'C:/Projects/NS&I/new_mockfiles/'\n",
    "batch28_output_filename = filepath + 'servicenow_vulnerability.parquet'\n",
    "num_samples = 3000\n",
    "\n",
    "length_min_days=1\n",
    "length_max_days=20\n",
    "random_days = pd.to_timedelta(np.random.randint(length_min_days, length_max_days, size=len(df)), unit='D')\n",
    "\n",
    "criticality_list = ['Low','Medium','High','Critical']\n",
    "\n",
    "ticket_id = ['TID' + f'{i:08}' for i in range(1, num_samples + 1)]\n",
    "\n",
    "start_date = pd.to_datetime('2023-09-01')\n",
    "end_date = pd.to_datetime(datetime.now().strftime('%Y-%m-%d')) \n",
    "created_timestamp = pd.to_datetime(np.random.randint(start_date.value, end_date.value, num_samples,dtype='int64'), unit='ns')\n",
    "closed_timestamp = created_timestamp + random_days\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'ticket_id':call_id,\n",
    "    'created_timestamp': created_timestamp,\n",
    "    'closed_timestamp': closed_timestamp,    \n",
    "})\n",
    "\n",
    "df['criticality'] = np.random.choice(criticality_list, size=len(df))\n",
    "df['created_timestamp'] = pd.to_datetime(df['created_timestamp']).dt.round('s')\n",
    "df['closed_timestamp'] = pd.to_datetime(df['closed_timestamp']).dt.round('s')\n",
    "\n",
    "\n",
    "df\n",
    "\n",
    "df.to_parquet(batch28_output_filename, engine='pyarrow', index=False)\n",
    "\n",
    "print(df.head())\n",
    "print(f\"Data saved to '{batch28_output_filename}'.\")\n",
    "print(df.dtypes)\n",
    "print('------------------------')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Mock Data\n",
    "\n",
    "Daily Batch Feed 29 servicenow_complaintresponse\n",
    "\n",
    "29 Includes the following columns:\n",
    "- case_id\n",
    "- case_type\n",
    "- resolved_timestamp\n",
    "- recourse_received_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       case_id  resolved_timestamp recourse_received_timestamp  \\\n",
      "0  CAS00000001 2024-01-03 12:47:49         2024-01-28 12:47:49   \n",
      "1  CAS00000002 2023-11-21 21:34:09         2023-12-11 21:34:09   \n",
      "2  CAS00000003 2023-09-23 01:29:35         2023-10-16 01:29:35   \n",
      "3  CAS00000004 2023-09-24 11:24:13         2023-10-17 11:24:13   \n",
      "4  CAS00000005 2023-11-06 13:50:35         2023-12-03 13:50:35   \n",
      "\n",
      "             case_type  \n",
      "0            Complaint  \n",
      "1  Online_Registration  \n",
      "2        Account_Query  \n",
      "3     Change_of_Detail  \n",
      "4        Account_Trace  \n",
      "Data saved to 'C:/Projects/NS&I/new_mockfiles/servicenow_complaintresponse.parquet'.\n",
      "case_id                                object\n",
      "resolved_timestamp             datetime64[ns]\n",
      "recourse_received_timestamp    datetime64[ns]\n",
      "case_type                              object\n",
      "dtype: object\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "# Mock data for Batch #29\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "### Create Batch 29\n",
    "# Set conditions\n",
    "np.random.seed(0)\n",
    "filepath = 'C:/Projects/NS&I/new_mockfiles/'\n",
    "batch29_output_filename = filepath + 'servicenow_complaintresponse.parquet'\n",
    "num_samples = 3000\n",
    "\n",
    "length_min_days=20\n",
    "length_max_days=31\n",
    "random_days = pd.to_timedelta(np.random.randint(length_min_days, length_max_days, size=len(df)), unit='D')\n",
    "\n",
    "case_type_list = ['Change_of_Detail','Subject_Access_Request','Sales_Application','Account_Trace',\n",
    "                         'Account_Query','Complaint','Telephony_Registration','Online_Registration','Bereavement']\n",
    "\n",
    "case_id = ['CAS' + f'{i:08}' for i in range(1, num_samples + 1)]\n",
    "\n",
    "start_date = pd.to_datetime('2023-09-01')\n",
    "end_date = pd.to_datetime(datetime.now().strftime('%Y-%m-%d')) \n",
    "resolved_timestamp = pd.to_datetime(np.random.randint(start_date.value, end_date.value, num_samples,dtype='int64'), unit='ns')\n",
    "recourse_received_timestamp = resolved_timestamp + random_days\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'case_id':case_id,\n",
    "    'resolved_timestamp': resolved_timestamp,\n",
    "    'recourse_received_timestamp': recourse_received_timestamp,    \n",
    "})\n",
    "\n",
    "df['case_type'] = np.random.choice(case_type_list, size=len(df))\n",
    "df['resolved_timestamp'] = pd.to_datetime(df['resolved_timestamp']).dt.round('s')\n",
    "df['recourse_received_timestamp'] = pd.to_datetime(df['recourse_received_timestamp']).dt.round('s')\n",
    "\n",
    "\n",
    "df\n",
    "\n",
    "df.to_parquet(batch29_output_filename, engine='pyarrow', index=False)\n",
    "\n",
    "print(df.head())\n",
    "print(f\"Data saved to '{batch29_output_filename}'.\")\n",
    "print(df.dtypes)\n",
    "print('------------------------')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Mock Data\n",
    "\n",
    "Daily Batch Feed 30 servicenow_resolvedissues\n",
    "\n",
    "30 Includes the following columns:\n",
    "- incident_id\n",
    "- time_to_action\n",
    "- priority\n",
    "- created_timestamp\n",
    "- resolved_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   incident_id   created_timestamp  resolved_timestamp  priority  \\\n",
      "0  INC00000001 2024-03-22 10:43:33 2024-03-22 12:43:33         2   \n",
      "1  INC00000002 2024-01-15 23:02:16 2024-01-16 02:02:16         2   \n",
      "2  INC00000003 2023-09-10 02:53:30 2023-09-10 10:53:30         3   \n",
      "3  INC00000004 2023-09-26 14:04:55 2023-09-26 17:04:55         2   \n",
      "4  INC00000005 2024-02-10 03:41:30 2024-02-26 20:41:30         4   \n",
      "\n",
      "   time_to_action  \n",
      "0             2.0  \n",
      "1             3.0  \n",
      "2             8.0  \n",
      "3             3.0  \n",
      "4           401.0  \n",
      "Data saved to 'C:/Projects/NS&I/new_mockfiles/servicenow_resolvedissues.parquet'.\n",
      "incident_id                   object\n",
      "created_timestamp     datetime64[ns]\n",
      "resolved_timestamp    datetime64[ns]\n",
      "priority                       int32\n",
      "time_to_action               float64\n",
      "dtype: object\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "# Mock data for Batch #30\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "### Create Batch 30\n",
    "# Set conditions\n",
    "np.random.seed(0)\n",
    "filepath = 'C:/Projects/NS&I/new_mockfiles/'\n",
    "batch30_output_filename = filepath + 'servicenow_resolvedissues.parquet'\n",
    "num_samples = 3000\n",
    "priority_list = [1,2,3,4]\n",
    "\n",
    "length_min_hrs=124\n",
    "length_max_hrs=720\n",
    "random_hrs = pd.to_timedelta(np.random.randint(length_min_hrs, length_max_hrs, size=len(df)), unit='h')\n",
    "\n",
    "incident_id = ['INC' + f'{i:08}' for i in range(1, num_samples + 1)]\n",
    "\n",
    "start_date = pd.to_datetime('2023-09-01')\n",
    "end_date = pd.to_datetime(datetime.now().strftime('%Y-%m-%d')) \n",
    "created_timestamp = pd.to_datetime(np.random.randint(start_date.value, end_date.value, num_samples,dtype='int64'), unit='ns')\n",
    "resolved_timestamp = created_timestamp\n",
    "time_to_action = random_hrs\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'incident_id': incident_id,\n",
    "    'created_timestamp': created_timestamp,\n",
    "    'resolved_timestamp': resolved_timestamp\n",
    "})\n",
    "\n",
    "df['priority'] = np.random.choice(priority_list, size=len(df))\n",
    "df['created_timestamp'] = pd.to_datetime(df['created_timestamp']).dt.round('s')\n",
    "df['resolved_timestamp'] = pd.to_datetime(df['resolved_timestamp']).dt.round('s')\n",
    "\n",
    "\n",
    "df.loc[df['priority'] == 4, 'resolved_timestamp'] = df['created_timestamp'] + random_hrs\n",
    "df.loc[df['priority'] == 3, 'resolved_timestamp'] = df['created_timestamp'] + pd.to_timedelta(np.random.randint(5, 24, size=len(df)), unit='h')\n",
    "df.loc[df['priority'] == 2, 'resolved_timestamp'] = df['created_timestamp'] + pd.to_timedelta(np.random.randint(2, 5, size=len(df)), unit='h')\n",
    "df.loc[df['priority'] == 1, 'resolved_timestamp'] = df['created_timestamp'] + pd.to_timedelta(np.random.randint(1, 3, size=len(df)), unit='h')\n",
    "df['time_to_action'] =(pd.to_datetime(df['resolved_timestamp']) - pd.to_datetime(df['created_timestamp'])).dt.total_seconds()/3600\n",
    "df\n",
    "df.to_parquet(batch30_output_filename, engine='pyarrow', index=False)\n",
    "\n",
    "print(df.head())\n",
    "print(f\"Data saved to '{batch30_output_filename}'.\")\n",
    "print(df.dtypes)\n",
    "print('------------------------')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Mock Data\n",
    "\n",
    "Daily Batch Feed 31 servicenow_deployedfixes\n",
    "\n",
    "31 Includes the following columns:\n",
    "- incident_id\n",
    "- time_to_action\n",
    "- priority\n",
    "- created_timestamp\n",
    "- deployed_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   incident_id   created_timestamp  deployed_timestamp  priority  \\\n",
      "0  INC00000001 2023-09-02 15:12:53 2023-10-26 15:12:53         2   \n",
      "1  INC00000002 2024-04-23 15:08:50 2024-06-19 15:08:50         2   \n",
      "2  INC00000003 2023-12-29 09:03:39 2024-03-01 09:03:39         4   \n",
      "3  INC00000004 2024-02-24 14:20:39 2024-03-05 14:20:39         3   \n",
      "4  INC00000005 2023-09-22 05:09:15 2023-10-05 05:09:15         2   \n",
      "\n",
      "   time_to_action  \n",
      "0              54  \n",
      "1              57  \n",
      "2              63  \n",
      "3              10  \n",
      "4              13  \n",
      "Data saved to 'C:/Projects/NS&I/new_mockfiles/servicenow_deployedfixes.parquet'.\n",
      "incident_id                   object\n",
      "created_timestamp     datetime64[ns]\n",
      "deployed_timestamp    datetime64[ns]\n",
      "priority                       int32\n",
      "time_to_action                 int64\n",
      "dtype: object\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "# Mock data for Batch #31\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "### Create Batch 31\n",
    "# Set conditions\n",
    "np.random.seed(0)\n",
    "filepath = 'C:/Projects/NS&I/new_mockfiles/'\n",
    "batch31_output_filename = filepath + 'servicenow_deployedfixes.parquet'\n",
    "num_samples = 3000\n",
    "priority_list = [1,2,3,4]\n",
    "\n",
    "length_min_days=10\n",
    "length_max_days=70\n",
    "random_days = pd.to_timedelta(np.random.randint(length_min_days, length_max_days, size=len(df)), unit='D')\n",
    "\n",
    "incident_id = ['INC' + f'{i:08}' for i in range(1, num_samples + 1)]\n",
    "\n",
    "start_date = pd.to_datetime('2023-09-01')\n",
    "end_date = pd.to_datetime(datetime.now().strftime('%Y-%m-%d')) \n",
    "created_timestamp = pd.to_datetime(np.random.randint(start_date.value, end_date.value, num_samples,dtype='int64'), unit='ns')\n",
    "deployed_timestamp = created_timestamp + random_days\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'incident_id': incident_id,\n",
    "    'created_timestamp': created_timestamp,\n",
    "    'deployed_timestamp': deployed_timestamp\n",
    "})\n",
    "\n",
    "df['priority'] = np.random.choice(priority_list, size=len(df))\n",
    "df['created_timestamp'] = pd.to_datetime(df['created_timestamp']).dt.round('s')\n",
    "df['deployed_timestamp'] = pd.to_datetime(df['deployed_timestamp']).dt.round('s')\n",
    "df['time_to_action'] =(pd.to_datetime(df['deployed_timestamp']) - pd.to_datetime(df['created_timestamp'])).dt.days\n",
    "df\n",
    "\n",
    "df.to_parquet(batch31_output_filename, engine='pyarrow', index=False)\n",
    "\n",
    "print(df.head())\n",
    "print(f\"Data saved to '{batch31_output_filename}'.\")\n",
    "print(df.dtypes)\n",
    "print('------------------------')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Mock Data\n",
    "\n",
    "Daily Batch Feed 32 servicenow_rca\n",
    "\n",
    "32 Includes the following columns:\n",
    "- incident_id\n",
    "- time_to_action\n",
    "- priority\n",
    "- created_timestamp\n",
    "- rca_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   incident_id   created_timestamp       rca_timestamp  priority  \\\n",
      "0  INC00000001 2024-02-26 22:43:10 2024-03-14 22:43:10         3   \n",
      "1  INC00000002 2024-03-22 11:18:34 2024-04-05 11:18:34         3   \n",
      "2  INC00000003 2024-04-14 09:03:14 2024-05-03 09:03:14         3   \n",
      "3  INC00000004 2023-10-04 13:41:23 2023-10-05 13:41:23         1   \n",
      "4  INC00000005 2023-12-18 12:13:59 2023-12-19 12:13:59         2   \n",
      "\n",
      "   time_to_action  \n",
      "0              17  \n",
      "1              14  \n",
      "2              19  \n",
      "3               1  \n",
      "4               1  \n",
      "Data saved to 'C:/Projects/NS&I/new_mockfiles/servicenow_rca.parquet'.\n",
      "incident_id                  object\n",
      "created_timestamp    datetime64[ns]\n",
      "rca_timestamp        datetime64[ns]\n",
      "priority                      int32\n",
      "time_to_action                int64\n",
      "dtype: object\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "# Mock data for Batch #32\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "### Create Batch 32\n",
    "# Set conditions\n",
    "np.random.seed(0)\n",
    "filepath = 'C:/Projects/NS&I/new_mockfiles/'\n",
    "batch32_output_filename = filepath + 'servicenow_rca.parquet'\n",
    "num_samples = 3000\n",
    "priority_list = [1,2,3,4]\n",
    "\n",
    "length_min_days=4\n",
    "length_max_days=35\n",
    "random_days = pd.to_timedelta(np.random.randint(length_min_days, length_max_days, size=len(df)), unit='D')\n",
    "\n",
    "incident_id = ['INC' + f'{i:08}' for i in range(1, num_samples + 1)]\n",
    "\n",
    "start_date = pd.to_datetime('2023-09-01')\n",
    "end_date = pd.to_datetime(datetime.now().strftime('%Y-%m-%d')) \n",
    "created_timestamp = pd.to_datetime(np.random.randint(start_date.value, end_date.value, num_samples,dtype='int64'), unit='ns')\n",
    "rca_timestamp = created_timestamp \n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'incident_id': incident_id,\n",
    "    'created_timestamp': created_timestamp,\n",
    "    'rca_timestamp': rca_timestamp\n",
    "})\n",
    "\n",
    "df['priority'] = np.random.choice(priority_list, size=len(df))\n",
    "df['created_timestamp'] = pd.to_datetime(df['created_timestamp']).dt.round('s')\n",
    "df['rca_timestamp'] = pd.to_datetime(df['rca_timestamp']).dt.round('s')\n",
    "\n",
    "\n",
    "df.loc[df['priority'] == 4, 'rca_timestamp'] = df['created_timestamp'] + random_hrs\n",
    "df.loc[df['priority'] == 3, 'rca_timestamp'] = df['created_timestamp'] + pd.to_timedelta(np.random.randint(3, 30, size=len(df)), unit='D')\n",
    "df.loc[df['priority'] == 2, 'rca_timestamp'] = df['created_timestamp'] + pd.to_timedelta(np.random.randint(1, 2, size=len(df)), unit='D')\n",
    "df.loc[df['priority'] == 1, 'rca_timestamp'] = df['created_timestamp'] + pd.to_timedelta(np.random.randint(1, 2, size=len(df)), unit='D')\n",
    "df['time_to_action'] =(pd.to_datetime(df['rca_timestamp']) - pd.to_datetime(df['created_timestamp'])).dt.days\n",
    "df\n",
    "df.to_parquet(batch32_output_filename, engine='pyarrow', index=False)\n",
    "\n",
    "print(df.head())\n",
    "print(f\"Data saved to '{batch32_output_filename}'.\")\n",
    "print(df.dtypes)\n",
    "print('------------------------')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Mock Data\n",
    "\n",
    "Daily Batch Feed 33 servicenow_permanentfixavailable\n",
    "\n",
    "33 Includes the following columns:\n",
    "- incident_id\n",
    "- time_to_action\n",
    "- created_timestamp\n",
    "- fix_available_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   incident_id   created_timestamp fix_available_timestamp  time_to_action\n",
      "0  INC00000001 2023-09-02 15:12:53     2023-10-26 15:12:53              54\n",
      "1  INC00000002 2024-04-23 15:08:50     2024-06-19 15:08:50              57\n",
      "2  INC00000003 2023-12-29 09:03:39     2024-03-01 09:03:39              63\n",
      "3  INC00000004 2024-02-24 14:20:39     2024-03-05 14:20:39              10\n",
      "4  INC00000005 2023-09-22 05:09:15     2023-10-05 05:09:15              13\n",
      "Data saved to 'C:/Projects/NS&I/new_mockfiles/servicenow_permanentfixavailable.parquet'.\n",
      "incident_id                        object\n",
      "created_timestamp          datetime64[ns]\n",
      "fix_available_timestamp    datetime64[ns]\n",
      "time_to_action                      int64\n",
      "dtype: object\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "# Mock data for Batch #33\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "### Create Batch 31\n",
    "# Set conditions\n",
    "np.random.seed(0)\n",
    "filepath = 'C:/Projects/NS&I/new_mockfiles/'\n",
    "batch33_output_filename = filepath + 'servicenow_permanentfixavailable.parquet'\n",
    "num_samples = 3000\n",
    "\n",
    "length_min_days=10\n",
    "length_max_days=70\n",
    "random_days = pd.to_timedelta(np.random.randint(length_min_days, length_max_days, size=len(df)), unit='D')\n",
    "\n",
    "incident_id = ['INC' + f'{i:08}' for i in range(1, num_samples + 1)]\n",
    "\n",
    "start_date = pd.to_datetime('2023-09-01')\n",
    "end_date = pd.to_datetime(datetime.now().strftime('%Y-%m-%d')) \n",
    "created_timestamp = pd.to_datetime(np.random.randint(start_date.value, end_date.value, num_samples,dtype='int64'), unit='ns')\n",
    "fix_available_timestamp = created_timestamp + random_days\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'incident_id': incident_id,\n",
    "    'created_timestamp': created_timestamp,\n",
    "    'fix_available_timestamp': fix_available_timestamp\n",
    "})\n",
    "\n",
    "df['created_timestamp'] = pd.to_datetime(df['created_timestamp']).dt.round('s')\n",
    "df['fix_available_timestamp'] = pd.to_datetime(df['fix_available_timestamp']).dt.round('s')\n",
    "df['time_to_action'] =(pd.to_datetime(df['fix_available_timestamp']) - pd.to_datetime(df['created_timestamp'])).dt.days\n",
    "df\n",
    "\n",
    "df.to_parquet(batch33_output_filename, engine='pyarrow', index=False)\n",
    "\n",
    "print(df.head())\n",
    "print(f\"Data saved to '{batch33_output_filename}'.\")\n",
    "print(df.dtypes)\n",
    "print('------------------------')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Mock Data\n",
    "\n",
    "Daily Batch Feed 34 servicenow_foscomplaint\n",
    "\n",
    "34 Includes the following columns:\n",
    "- case_id\n",
    "- created_timestamp\n",
    "- fos_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       case_id   created_timestamp       fos_timestamp\n",
      "0  CAS00000001 2023-12-12 10:01:47 2023-12-29 10:01:47\n",
      "1  CAS00000002 2024-02-01 00:02:41 2024-02-11 00:02:41\n",
      "2  CAS00000003 2023-09-25 02:07:04 2023-09-30 02:07:04\n",
      "3  CAS00000004 2024-01-29 00:42:27 2024-02-06 00:42:27\n",
      "4  CAS00000005 2023-09-02 15:12:53 2023-09-18 15:12:53\n",
      "Data saved to 'C:/Projects/NS&I/new_mockfiles/servicenow_foscomplaint.parquet'.\n",
      "case_id                      object\n",
      "created_timestamp    datetime64[ns]\n",
      "fos_timestamp        datetime64[ns]\n",
      "dtype: object\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "# Mock data for Batch #34\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "### Create Batch 31\n",
    "# Set conditions\n",
    "np.random.seed(0)\n",
    "filepath = 'C:/Projects/NS&I/new_mockfiles/'\n",
    "batch34_output_filename = filepath + 'servicenow_foscomplaint.parquet'\n",
    "num_samples = 3000\n",
    "\n",
    "length_min_days=5\n",
    "length_max_days=20\n",
    "random_days = pd.to_timedelta(np.random.randint(length_min_days, length_max_days, size=len(df)), unit='D')\n",
    "\n",
    "case_id = ['CAS' + f'{i:08}' for i in range(1, num_samples + 1)]\n",
    "\n",
    "start_date = pd.to_datetime('2023-09-01')\n",
    "end_date = pd.to_datetime(datetime.now().strftime('%Y-%m-%d')) \n",
    "created_timestamp = pd.to_datetime(np.random.randint(start_date.value, end_date.value, num_samples,dtype='int64'), unit='ns')\n",
    "fos_timestamp = created_timestamp + random_days\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'case_id': case_id,\n",
    "    'created_timestamp': created_timestamp,\n",
    "    'fos_timestamp': fos_timestamp\n",
    "})\n",
    "\n",
    "df['created_timestamp'] = pd.to_datetime(df['created_timestamp']).dt.round('s')\n",
    "df['fos_timestamp'] = pd.to_datetime(df['fos_timestamp']).dt.round('s')\n",
    "df\n",
    "\n",
    "df.to_parquet(batch34_output_filename, engine='pyarrow', index=False)\n",
    "\n",
    "print(df.head())\n",
    "print(f\"Data saved to '{batch34_output_filename}'.\")\n",
    "print(df.dtypes)\n",
    "print('------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
